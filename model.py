# -*- coding: utf-8 -*-
"""Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_aEzWMM6_DHirYwGYusrjPZoG_eZZBAO
"""

import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow
from tensorflow.keras import layers
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import glob
from keras.models import load_model
import ntpath
from sklearn.metrics import confusion_matrix
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pickle

from google.colab import drive
drive.mount('/content/drive/')

image_size = [224, 224]

training_path = os.listdir('/content/drive/MyDrive/Dataset/Disease/Training')
testing_path = os.listdir('/content/drive/MyDrive/Dataset/Disease/Testing')

inception = InceptionV3(input_shape = image_size + [3], include_top=False)

for layer in inception.layers:
    layer.trainable = False

trainfile = glob.glob('/content/drive/MyDrive/Dataset/Disease/Training/*')
print(len(trainfile))

print("Creating Model")
model = Sequential([
    inception,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(rate=0.2),
    Dense(len(trainfile), activation='softmax')
])


model.summary()

model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)

training_datagen = ImageDataGenerator(rescale = 1./255,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

testing_datagen = ImageDataGenerator(rescale = 1./255)

training_set = training_datagen.flow_from_directory('/content/drive/MyDrive/Dataset/Disease/Training',
                                                 target_size = (224, 224),
                                                 batch_size = 50,
                                                 class_mode = 'categorical')

testing_set = testing_datagen.flow_from_directory('/content/drive/MyDrive/Dataset/Disease/Testing',
                                            target_size = (224, 224),
                                            batch_size = 50,
                                            class_mode = 'categorical')

file = '/content/drive/MyDrive/Dataset/Disease'
check = ModelCheckpoint(file, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
early = EarlyStopping(monitor="accuracy", mode="max", patience=15)

callbacks_list = [check, early]

history = model.fit(
  training_set,
  validation_data=testing_set,
  epochs=20,
  steps_per_epoch=len(training_set),
  validation_steps=len(testing_set),
  callbacks=callbacks_list
)

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(accuracy))

plt.plot(epochs, accuracy, 'r', "Training Accuracy")
plt.plot(epochs, val_accuracy, 'b', "Validation Accuracy")
plt.title('Training and Validation Accuracy')
plt.show()
print("")

plt.plot(epochs, loss, 'r', "Training Loss")
plt.plot(epochs, val_loss, 'b', "Validation Loss")
plt.title('Training and Validation Loss')
plt.show()
plt.show()
print("")

print(training_set)

training_set.class_indices

batch_size = 50
files = '/content/drive/MyDrive/Dataset/Disease'
densenet_model = tensorflow.keras.applications.DenseNet121(weights='imagenet')

from IPython import display
from google.colab import files
import numpy as np
from keras.preprocessing import image
import matplotlib.pyplot as plt

uploaded = files.upload()

for fn in uploaded.keys():
  files = fn
  img = tensorflow.keras.preprocessing.image.load_img(files, target_size = image_size)
  imgplot = plt.imshow(img)
  x = tensorflow.keras.preprocessing.image.img_to_array(img)
  x = np.expand_dims(x, axis = 0)

  images = np.vstack([x])
  classes = densenet_model.predict(images, batch_size = batch_size)
  classes = np.argmax(classes[0])

  print(fn)
  if classes==0:
    print('Ear Mites')
  elif classes==1:
    print('Flea')
  elif classes==2:
    print('Leprosy')
  elif classes==3:
    print('Pyoderma')
  else:
    print('Ringworm')

import tensorflow as tf
model.save("/content/drive/MyDrive/Dataset/Disease/Model.h5")

export_dir = 'saved_model/1'
tf.saved_model.save(model, export_dir = export_dir)

mode = "Speed"

if mode == 'storage':
  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE
elif mode == 'Speed':
  optimization = tf.lite.Optimize.DEFAULT
else:
  optimization = tf.lite.Optimize.DEFAULT

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)

converter.optimizations = [optimization]

tflite_model = converter.convert();

import pathlib
tflite_model_file = '/content/drive/MyDrive/Dataset/Disease/model.tflite'
with open(tflite_model_file, 'wb') as f:
    f.write(tflite_model)